/**
 * LingoRecall AI - Local Model Service
 * æœ¬åœ°æ¨¡å‹æœåŠ¡ï¼šè¿æ¥æµ‹è¯•ã€æ¨¡å‹å‘ç°ã€ç¡¬ä»¶æ£€æµ‹å’Œæ¨¡å‹æ¨è
 *
 * @module services/localModelService
 */

import type { LocalModelTool } from '../shared/types/settings';

// ============================================================
// Types
// ============================================================

/**
 * æœ¬åœ°å·¥å…·é…ç½®
 */
export interface LocalToolConfig {
  name: string;
  defaultEndpoint: string;
  modelListEndpoint?: string;
  chatEndpoint: string;
  installUrl: string;
  installInstructions: Record<'mac' | 'windows' | 'linux', string[]>;
}

/**
 * è¿æ¥æµ‹è¯•ç»“æœ
 */
export interface ConnectionTestResult {
  success: boolean;
  latencyMs?: number;
  availableModels?: string[];
  error?: string;
  errorCode?: 'CONNECTION_REFUSED' | 'TIMEOUT' | 'INVALID_RESPONSE' | 'MODEL_NOT_FOUND';
}

/**
 * ç¡¬ä»¶æ£€æµ‹ç»“æœ
 */
export interface HardwareInfo {
  platform: 'mac' | 'windows' | 'linux' | 'unknown';
  isAppleSilicon: boolean;
  estimatedRam: number;
  gpuType: 'nvidia' | 'amd' | 'apple' | 'integrated' | 'unknown';
}

/**
 * æ¨¡å‹æ¨è
 */
export interface ModelRecommendation {
  modelName: string;
  displayName: string;
  sizeGB: number;
  ramRequired: number;
  speed: 'fast' | 'medium' | 'slow';
  quality: 'high' | 'medium' | 'basic';
  useCase: string;
  recommended?: boolean;
}

// ============================================================
// Constants
// ============================================================

/**
 * æœ¬åœ°æ¨¡å‹å·¥å…·é…ç½®
 */
export const LOCAL_TOOLS: Record<LocalModelTool, LocalToolConfig> = {
  ollama: {
    name: 'Ollama',
    defaultEndpoint: 'http://localhost:11434',
    modelListEndpoint: '/api/tags',
    chatEndpoint: '/api/chat',
    installUrl: 'https://ollama.ai/download',
    installInstructions: {
      mac: [
        '1. è®¿é—® ollama.ai/download ä¸‹è½½ Ollama',
        '2. åŒå‡» .dmg æ–‡ä»¶å®‰è£…',
        '3. æ‰“å¼€ç»ˆç«¯ï¼Œä¸‹è½½æ¨èæ¨¡å‹:',
        '   â€¢ ç¿»è¯‘ä¸“ç”¨(æ¨è): ollama pull translategemma:12b',
        '   â€¢ é€šç”¨æ¨¡å‹: ollama pull llama3.2',
        '4. æ¨¡å‹ä¸‹è½½å®Œæˆåå³å¯ä½¿ç”¨',
      ],
      windows: [
        '1. è®¿é—® ollama.ai/download ä¸‹è½½ Windows ç‰ˆæœ¬',
        '2. è¿è¡Œå®‰è£…ç¨‹åº',
        '3. æ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼Œä¸‹è½½æ¨èæ¨¡å‹:',
        '   â€¢ ç¿»è¯‘ä¸“ç”¨(æ¨è): ollama pull translategemma:12b',
        '   â€¢ é€šç”¨æ¨¡å‹: ollama pull llama3.2',
        '4. æ¨¡å‹ä¸‹è½½å®Œæˆåå³å¯ä½¿ç”¨',
      ],
      linux: [
        '1. è¿è¡Œ: curl -fsSL https://ollama.ai/install.sh | sh',
        '2. ä¸‹è½½æ¨èæ¨¡å‹:',
        '   â€¢ ç¿»è¯‘ä¸“ç”¨(æ¨è): ollama pull translategemma:12b',
        '   â€¢ é€šç”¨æ¨¡å‹: ollama pull llama3.2',
        '3. æ¨¡å‹ä¸‹è½½å®Œæˆåå³å¯ä½¿ç”¨',
      ],
    },
  },
  'lm-studio': {
    name: 'LM Studio',
    defaultEndpoint: 'http://localhost:1234',
    chatEndpoint: '/v1/chat/completions',
    installUrl: 'https://lmstudio.ai/',
    installInstructions: {
      mac: [
        '1. è®¿é—® lmstudio.ai ä¸‹è½½ LM Studio',
        '2. å®‰è£…å¹¶æ‰“å¼€ LM Studio',
        '3. åœ¨ Discover é¡µé¢æœç´¢å¹¶ä¸‹è½½æ¨¡å‹',
        '4. åˆ‡æ¢åˆ° Local Server æ ‡ç­¾é¡µï¼Œç‚¹å‡» Start Server',
      ],
      windows: [
        '1. è®¿é—® lmstudio.ai ä¸‹è½½ LM Studio',
        '2. å®‰è£…å¹¶æ‰“å¼€ LM Studio',
        '3. åœ¨ Discover é¡µé¢æœç´¢å¹¶ä¸‹è½½æ¨¡å‹',
        '4. åˆ‡æ¢åˆ° Local Server æ ‡ç­¾é¡µï¼Œç‚¹å‡» Start Server',
      ],
      linux: [
        '1. è®¿é—® lmstudio.ai ä¸‹è½½ AppImage',
        '2. è¿è¡Œ AppImage æ–‡ä»¶',
        '3. ä¸‹è½½æ¨¡å‹å¹¶å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨',
      ],
    },
  },
  'llama-cpp': {
    name: 'llama.cpp',
    defaultEndpoint: 'http://localhost:8080',
    chatEndpoint: '/v1/chat/completions',
    installUrl: 'https://github.com/ggerganov/llama.cpp',
    installInstructions: {
      mac: [
        '1. å®‰è£… Homebrewï¼ˆå¦‚æœªå®‰è£…ï¼‰',
        '2. è¿è¡Œ: brew install llama.cpp',
        '3. ä¸‹è½½ GGUF æ ¼å¼æ¨¡å‹æ–‡ä»¶',
        '4. è¿è¡Œ: llama-server -m model.gguf --port 8080',
      ],
      windows: [
        '1. ä» GitHub Releases ä¸‹è½½é¢„ç¼–è¯‘ç‰ˆæœ¬',
        '2. ä¸‹è½½ GGUF æ ¼å¼æ¨¡å‹æ–‡ä»¶',
        '3. è¿è¡Œ: llama-server.exe -m model.gguf --port 8080',
      ],
      linux: [
        '1. å…‹éš†ä»“åº“: git clone https://github.com/ggerganov/llama.cpp',
        '2. ç¼–è¯‘: cd llama.cpp && make',
        '3. ä¸‹è½½ GGUF æ ¼å¼æ¨¡å‹æ–‡ä»¶',
        '4. è¿è¡Œ: ./llama-server -m model.gguf --port 8080',
      ],
    },
  },
  other: {
    name: 'å…¶ä»–',
    defaultEndpoint: 'http://localhost:8000',
    chatEndpoint: '/v1/chat/completions',
    installUrl: '',
    installInstructions: {
      mac: ['è¯·å‚è€ƒå¯¹åº”å·¥å…·çš„å®˜æ–¹æ–‡æ¡£é…ç½®æœ¬åœ°æœåŠ¡å™¨'],
      windows: ['è¯·å‚è€ƒå¯¹åº”å·¥å…·çš„å®˜æ–¹æ–‡æ¡£é…ç½®æœ¬åœ°æœåŠ¡å™¨'],
      linux: ['è¯·å‚è€ƒå¯¹åº”å·¥å…·çš„å®˜æ–¹æ–‡æ¡£é…ç½®æœ¬åœ°æœåŠ¡å™¨'],
    },
  },
};

/**
 * æ¨¡å‹æ¨èåˆ—è¡¨
 *
 * åˆ†ä¸ºä¸¤ç±»ï¼š
 * 1. ç¿»è¯‘ä¸“ç”¨æ¨¡å‹ (TranslateGemma) - Google ä¸“é—¨è®­ç»ƒçš„ç¿»è¯‘æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ã€ç¿»è¯‘è´¨é‡é«˜
 * 2. é€šç”¨æ¨¡å‹ - å¯ä»¥åšç¿»è¯‘ã€å•è¯åˆ†æç­‰å¤šç§ä»»åŠ¡
 */
export const MODEL_RECOMMENDATIONS: ModelRecommendation[] = [
  // ============================================================
  // ğŸŒŸ TranslateGemma ç³»åˆ— - ç¿»è¯‘ä¸“ç”¨æ¨¡å‹ï¼ˆå¼ºçƒˆæ¨èï¼‰
  // Google 2025å¹´1æœˆå‘å¸ƒçš„å¼€æºç¿»è¯‘æ¨¡å‹ï¼ŒåŸºäº Gemma 3 æ¶æ„
  // ç‰¹ç‚¹ï¼šç¿»è¯‘é€Ÿåº¦å¿«ï¼ˆæ¯”é€šç”¨æ¨¡å‹å¿« 5-8 å€ï¼‰ã€æ”¯æŒ 55 ç§è¯­è¨€ã€ç¿»è¯‘è´¨é‡é«˜
  // ============================================================
  {
    modelName: 'translategemma:4b',
    displayName: 'TranslateGemma 4B âš¡',
    sizeGB: 3.3,
    ramRequired: 8,
    speed: 'fast',
    quality: 'high',
    useCase: 'ğŸŒŸ ç¿»è¯‘ä¸“ç”¨ï¼šé€Ÿåº¦æœ€å¿«ï¼Œ8GB å†…å­˜æ¨è',
  },
  {
    modelName: 'translategemma:12b',
    displayName: 'TranslateGemma 12B ğŸ†',
    sizeGB: 8.1,
    ramRequired: 16,
    speed: 'fast',
    quality: 'high',
    useCase: 'ğŸ† å¼ºçƒˆæ¨èï¼šä¸“ä¸šç¿»è¯‘è´¨é‡ï¼Œé€Ÿåº¦å¿«',
    recommended: true,
  },
  {
    modelName: 'translategemma:27b',
    displayName: 'TranslateGemma 27B',
    sizeGB: 17,
    ramRequired: 32,
    speed: 'medium',
    quality: 'high',
    useCase: 'æœ€é«˜ç¿»è¯‘è´¨é‡ï¼Œéœ€è¦ 32GB å†…å­˜',
  },

  // ============================================================
  // é€šç”¨æ¨¡å‹ - å¯ä»¥åšç¿»è¯‘ã€å•è¯åˆ†æç­‰ä»»åŠ¡
  // ============================================================
  {
    modelName: 'llama3.2:1b',
    displayName: 'Llama 3.2 1B',
    sizeGB: 1.3,
    ramRequired: 4,
    speed: 'fast',
    quality: 'basic',
    useCase: 'ä½é…è®¾å¤‡ï¼Œå¿«é€Ÿå“åº”',
  },
  {
    modelName: 'llama3.2:3b',
    displayName: 'Llama 3.2 3B',
    sizeGB: 2.0,
    ramRequired: 6,
    speed: 'fast',
    quality: 'medium',
    useCase: 'å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡',
  },
  {
    modelName: 'llama3.2',
    displayName: 'Llama 3.2 8B',
    sizeGB: 4.7,
    ramRequired: 8,
    speed: 'medium',
    quality: 'high',
    useCase: 'é€šç”¨æ¨¡å‹ï¼šæ€§ä»·æ¯”é«˜',
  },
  {
    modelName: 'qwen2.5:7b',
    displayName: 'Qwen 2.5 7B',
    sizeGB: 4.4,
    ramRequired: 8,
    speed: 'medium',
    quality: 'high',
    useCase: 'ä¸­æ–‡ä¼˜åŒ–ï¼Œç¿»è¯‘è´¨é‡é«˜',
  },
  {
    modelName: 'gemma2:9b',
    displayName: 'Gemma 2 9B',
    sizeGB: 5.4,
    ramRequired: 10,
    speed: 'medium',
    quality: 'high',
    useCase: 'Google å¼€æºæ¨¡å‹',
  },
  {
    modelName: 'mistral',
    displayName: 'Mistral 7B',
    sizeGB: 4.1,
    ramRequired: 8,
    speed: 'fast',
    quality: 'high',
    useCase: 'é€Ÿåº¦ä¸è´¨é‡å…¼é¡¾',
  },
];

// ============================================================
// Hardware Detection
// ============================================================

/**
 * æ£€æµ‹ç”¨æˆ·ç¡¬ä»¶
 */
export function detectHardware(): HardwareInfo {
  const platform = navigator.platform || '';
  const userAgent = navigator.userAgent || '';

  // æ£€æµ‹æ“ä½œç³»ç»Ÿ
  let os: HardwareInfo['platform'] = 'unknown';
  if (platform.includes('Mac') || userAgent.includes('Mac')) {
    os = 'mac';
  } else if (platform.includes('Win') || userAgent.includes('Windows')) {
    os = 'windows';
  } else if (platform.includes('Linux') || userAgent.includes('Linux')) {
    os = 'linux';
  }

  // æ£€æµ‹ Apple Silicon
  const isAppleSilicon =
    os === 'mac' &&
    (userAgent.includes('ARM') ||
      // @ts-expect-error - userAgentData is experimental
      navigator.userAgentData?.platform === 'macOS');

  // ä¼°ç®— RAMï¼ˆæµè§ˆå™¨é™åˆ¶ï¼Œåªèƒ½ç²—ç•¥ä¼°è®¡ï¼‰
  // @ts-expect-error - deviceMemory is experimental
  const estimatedRam: number = navigator.deviceMemory || 8;

  // GPU ç±»å‹æ£€æµ‹
  let gpuType: HardwareInfo['gpuType'] = 'unknown';
  try {
    const canvas = document.createElement('canvas');
    const gl = canvas.getContext('webgl') || canvas.getContext('experimental-webgl');
    if (gl) {
      const debugInfo = (gl as WebGLRenderingContext).getExtension('WEBGL_debug_renderer_info');
      if (debugInfo) {
        const renderer = (gl as WebGLRenderingContext).getParameter(
          debugInfo.UNMASKED_RENDERER_WEBGL
        );
        if (renderer.includes('NVIDIA')) gpuType = 'nvidia';
        else if (renderer.includes('AMD') || renderer.includes('Radeon')) gpuType = 'amd';
        else if (renderer.includes('Apple')) gpuType = 'apple';
        else if (renderer.includes('Intel')) gpuType = 'integrated';
      }
    }
  } catch {
    // GPU detection failed, keep as unknown
  }

  return {
    platform: os,
    isAppleSilicon,
    estimatedRam,
    gpuType,
  };
}

/**
 * æ ¹æ®ç¡¬ä»¶æ¨èæ¨¡å‹
 */
export function getModelRecommendations(hardware: HardwareInfo): ModelRecommendation[] {
  const ram = hardware.estimatedRam || 8;

  return MODEL_RECOMMENDATIONS.filter((model) => model.ramRequired <= ram).sort((a, b) => {
    // Apple Silicon æˆ– NVIDIA GPU ç”¨æˆ·ä¼˜å…ˆæ¨èè¾ƒå¤§æ¨¡å‹
    if (hardware.isAppleSilicon || hardware.gpuType === 'nvidia') {
      // ä¼˜å…ˆæ¨è recommended çš„æ¨¡å‹
      if (a.recommended && !b.recommended) return -1;
      if (!a.recommended && b.recommended) return 1;
      return b.sizeGB - a.sizeGB;
    }

    // å…¶ä»–æƒ…å†µä¼˜å…ˆæ¨èå¹³è¡¡æ¨¡å‹
    const qualityScore = { high: 3, medium: 2, basic: 1 };
    const speedScore = { fast: 3, medium: 2, slow: 1 };

    // ä¼˜å…ˆæ¨è recommended çš„æ¨¡å‹
    if (a.recommended && !b.recommended) return -1;
    if (!a.recommended && b.recommended) return 1;

    return (
      qualityScore[b.quality] +
      speedScore[b.speed] -
      (qualityScore[a.quality] + speedScore[a.speed])
    );
  });
}

// ============================================================
// Connection Testing
// ============================================================

/**
 * æµ‹è¯•æœ¬åœ°æ¨¡å‹è¿æ¥
 */
export async function testLocalConnection(
  endpoint: string,
  modelName: string,
  tool: LocalModelTool
): Promise<ConnectionTestResult> {
  const startTime = performance.now();
  const toolConfig = LOCAL_TOOLS[tool];

  try {
    // 1. é¦–å…ˆæ£€æŸ¥æœåŠ¡æ˜¯å¦åœ¨è¿è¡Œ
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 5000);

    try {
      await fetch(endpoint, {
        method: 'GET',
        signal: controller.signal,
      });
      clearTimeout(timeoutId);
    } catch (error) {
      clearTimeout(timeoutId);
      if (error instanceof Error && error.name === 'AbortError') {
        return {
          success: false,
          error: `è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ ${toolConfig.name} æ˜¯å¦æ­£åœ¨è¿è¡Œ`,
          errorCode: 'TIMEOUT',
        };
      }
      return {
        success: false,
        error: `æ— æ³•è¿æ¥åˆ° ${toolConfig.name}ï¼Œè¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œ`,
        errorCode: 'CONNECTION_REFUSED',
      };
    }

    // 2. è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ (Ollama specific)
    let availableModels: string[] = [];
    if (tool === 'ollama' && toolConfig.modelListEndpoint) {
      try {
        const modelsResponse = await fetch(`${endpoint}${toolConfig.modelListEndpoint}`);
        const modelsData = await modelsResponse.json();
        availableModels = modelsData.models?.map((m: { name: string }) => m.name) || [];
      } catch {
        // Model list fetch failed, continue without it
      }
    }

    // 3. å‘é€æµ‹è¯•è¯·æ±‚
    const testEndpoint =
      tool === 'ollama' ? `${endpoint}/api/chat` : `${endpoint}${toolConfig.chatEndpoint}`;

    const testRequest =
      tool === 'ollama'
        ? {
            model: modelName,
            messages: [{ role: 'user', content: 'Say OK' }],
            stream: false,
          }
        : {
            model: modelName,
            messages: [{ role: 'user', content: 'Say OK' }],
            max_tokens: 10,
          };

    const testController = new AbortController();
    const testTimeoutId = setTimeout(() => testController.abort(), 30000);

    try {
      const response = await fetch(testEndpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(testRequest),
        signal: testController.signal,
      });
      clearTimeout(testTimeoutId);

      if (!response.ok) {
        const errorText = await response.text();
        if (response.status === 404 || errorText.includes('not found')) {
          return {
            success: false,
            error: `æœªæ‰¾åˆ°æ¨¡å‹ "${modelName}"ï¼Œè¯·å…ˆä¸‹è½½è¯¥æ¨¡å‹`,
            errorCode: 'MODEL_NOT_FOUND',
            availableModels,
          };
        }
        return {
          success: false,
          error: `æœåŠ¡å™¨é”™è¯¯: ${response.status}`,
          errorCode: 'INVALID_RESPONSE',
          availableModels,
        };
      }

      const latencyMs = Math.round(performance.now() - startTime);

      return {
        success: true,
        latencyMs,
        availableModels,
      };
    } catch (error) {
      clearTimeout(testTimeoutId);
      if (error instanceof Error && error.name === 'AbortError') {
        return {
          success: false,
          error: 'æ¨¡å‹å“åº”è¶…æ—¶ï¼Œå¯èƒ½æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•',
          errorCode: 'TIMEOUT',
          availableModels,
        };
      }
      throw error;
    }
  } catch (error) {
    if (error instanceof Error) {
      if (error.message.includes('Failed to fetch') || error.message.includes('NetworkError')) {
        return {
          success: false,
          error: `æ— æ³•è¿æ¥åˆ° ${toolConfig.name}`,
          errorCode: 'CONNECTION_REFUSED',
        };
      }
    }
    return {
      success: false,
      error: 'å‘ç”ŸæœªçŸ¥é”™è¯¯',
      errorCode: 'INVALID_RESPONSE',
    };
  }
}

/**
 * è·å–å·²å®‰è£…çš„æ¨¡å‹åˆ—è¡¨ï¼ˆä»… Ollamaï¼‰
 */
export async function getAvailableModels(
  endpoint: string,
  tool: LocalModelTool
): Promise<string[]> {
  if (tool !== 'ollama') {
    return [];
  }

  try {
    const response = await fetch(`${endpoint}/api/tags`);
    if (!response.ok) {
      return [];
    }
    const data = await response.json();
    return data.models?.map((m: { name: string }) => m.name) || [];
  } catch {
    return [];
  }
}

/**
 * è·å–å®‰è£…æŒ‡å—
 */
export function getInstallInstructions(
  tool: LocalModelTool,
  platform: HardwareInfo['platform']
): string[] {
  const toolConfig = LOCAL_TOOLS[tool];
  const platformKey = platform === 'unknown' ? 'mac' : platform;
  return toolConfig.installInstructions[platformKey];
}
